// Generated by the gRPC protobuf plugin.
// If you make any local change, they will be lost.
// source: google/cloud/ml/v1beta1/prediction_service.proto
// Original file comments:
// Copyright 2016 Google Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
#ifndef GRPC_google_2fcloud_2fml_2fv1beta1_2fprediction_5fservice_2eproto__INCLUDED
#define GRPC_google_2fcloud_2fml_2fv1beta1_2fprediction_5fservice_2eproto__INCLUDED

#include "google/cloud/ml/v1beta1/prediction_service.pb.h"

#include <grpc++/impl/codegen/async_stream.h>
#include <grpc++/impl/codegen/async_unary_call.h>
#include <grpc++/impl/codegen/method_handler_impl.h>
#include <grpc++/impl/codegen/proto_utils.h>
#include <grpc++/impl/codegen/rpc_method.h>
#include <grpc++/impl/codegen/service_type.h>
#include <grpc++/impl/codegen/status.h>
#include <grpc++/impl/codegen/stub_options.h>
#include <grpc++/impl/codegen/sync_stream.h>

namespace grpc {
class CompletionQueue;
class Channel;
class RpcService;
class ServerCompletionQueue;
class ServerContext;
}  // namespace grpc

namespace google {
namespace cloud {
namespace ml {
namespace v1beta1 {

// Copyright 2016 Google Inc. All Rights Reserved.
//
// Proto file for the Prediction service, both online and batch prediction.
//
// The Prediction API, which serves predictions for models managed by
// ModelService.
class OnlinePredictionService final {
 public:
  class StubInterface {
   public:
    virtual ~StubInterface() {}
    // Performs prediction on the data in the request.
    //
    // Responses are very similar to requests. There are two top-level fields,
    // each of which are JSON lists:
    //
    // <dl>
    //   <dt>predictions</dt>
    //   <dd>The list of predictions, one per instance in the request.</dd>
    //   <dt>error</dt>
    //   <dd>An error message returned instead of a prediction list if any
    //       instance produced an error.</dd>
    // </dl>
    //
    // If the call is successful, the response body will contain one prediction
    // entry per instance in the request body. If prediction fails for any
    // instance, the response body will contain no predictions and will contian
    // a single error entry instead.
    //
    // Even though there is one prediction per instance, the format of a
    // prediction is not directly related to the format of an instance.
    // Predictions take whatever format is specified in the outputs collection
    // defined in the model. The collection of predictions is returned in a JSON
    // list. Each member of the list can be a simple value, a list, or a JSON
    // object of any complexity. If your model has more than one output tensor,
    // each prediction will be a JSON object containing a name/value pair for each
    // output. The names identify the output aliases in the graph.
    //
    // The following examples show some possible responses:
    //
    // A simple set of predictions for three input instances, where each
    // prediction is an integer value:
    // <pre>
    // {"predictions": [5, 4, 3]}
    // </pre>
    // A more complex set of predictions, each containing two named values that
    // correspond to output tensors, named **label** and **scores** respectively.
    // The value of **label** is the predicted category ("car" or "beach") and
    // **scores** contains a list of probabilities for that instance across the
    // possible categories.
    // <pre>
    // {"predictions": [{"label": "beach", "scores": [0.1, 0.9]},
    //                  {"label": "car", "scores": [0.75, 0.25]}]}
    // </pre>
    // A response when there is an error processing an input instance:
    // <pre>
    // {"error": "Divide by zero"}
    // </pre>
    virtual ::grpc::Status Predict(::grpc::ClientContext* context, const ::google::cloud::ml::v1beta1::PredictRequest& request, ::google::api::HttpBody* response) = 0;
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::google::api::HttpBody>> AsyncPredict(::grpc::ClientContext* context, const ::google::cloud::ml::v1beta1::PredictRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::google::api::HttpBody>>(AsyncPredictRaw(context, request, cq));
    }
  private:
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::google::api::HttpBody>* AsyncPredictRaw(::grpc::ClientContext* context, const ::google::cloud::ml::v1beta1::PredictRequest& request, ::grpc::CompletionQueue* cq) = 0;
  };
  class Stub final : public StubInterface {
   public:
    Stub(const std::shared_ptr< ::grpc::ChannelInterface>& channel);
    ::grpc::Status Predict(::grpc::ClientContext* context, const ::google::cloud::ml::v1beta1::PredictRequest& request, ::google::api::HttpBody* response) override;
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::google::api::HttpBody>> AsyncPredict(::grpc::ClientContext* context, const ::google::cloud::ml::v1beta1::PredictRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::google::api::HttpBody>>(AsyncPredictRaw(context, request, cq));
    }

   private:
    std::shared_ptr< ::grpc::ChannelInterface> channel_;
    ::grpc::ClientAsyncResponseReader< ::google::api::HttpBody>* AsyncPredictRaw(::grpc::ClientContext* context, const ::google::cloud::ml::v1beta1::PredictRequest& request, ::grpc::CompletionQueue* cq) override;
    const ::grpc::RpcMethod rpcmethod_Predict_;
  };
  static std::unique_ptr<Stub> NewStub(const std::shared_ptr< ::grpc::ChannelInterface>& channel, const ::grpc::StubOptions& options = ::grpc::StubOptions());

  class Service : public ::grpc::Service {
   public:
    Service();
    virtual ~Service();
    // Performs prediction on the data in the request.
    //
    // Responses are very similar to requests. There are two top-level fields,
    // each of which are JSON lists:
    //
    // <dl>
    //   <dt>predictions</dt>
    //   <dd>The list of predictions, one per instance in the request.</dd>
    //   <dt>error</dt>
    //   <dd>An error message returned instead of a prediction list if any
    //       instance produced an error.</dd>
    // </dl>
    //
    // If the call is successful, the response body will contain one prediction
    // entry per instance in the request body. If prediction fails for any
    // instance, the response body will contain no predictions and will contian
    // a single error entry instead.
    //
    // Even though there is one prediction per instance, the format of a
    // prediction is not directly related to the format of an instance.
    // Predictions take whatever format is specified in the outputs collection
    // defined in the model. The collection of predictions is returned in a JSON
    // list. Each member of the list can be a simple value, a list, or a JSON
    // object of any complexity. If your model has more than one output tensor,
    // each prediction will be a JSON object containing a name/value pair for each
    // output. The names identify the output aliases in the graph.
    //
    // The following examples show some possible responses:
    //
    // A simple set of predictions for three input instances, where each
    // prediction is an integer value:
    // <pre>
    // {"predictions": [5, 4, 3]}
    // </pre>
    // A more complex set of predictions, each containing two named values that
    // correspond to output tensors, named **label** and **scores** respectively.
    // The value of **label** is the predicted category ("car" or "beach") and
    // **scores** contains a list of probabilities for that instance across the
    // possible categories.
    // <pre>
    // {"predictions": [{"label": "beach", "scores": [0.1, 0.9]},
    //                  {"label": "car", "scores": [0.75, 0.25]}]}
    // </pre>
    // A response when there is an error processing an input instance:
    // <pre>
    // {"error": "Divide by zero"}
    // </pre>
    virtual ::grpc::Status Predict(::grpc::ServerContext* context, const ::google::cloud::ml::v1beta1::PredictRequest* request, ::google::api::HttpBody* response);
  };
  template <class BaseClass>
  class WithAsyncMethod_Predict : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service *service) {}
   public:
    WithAsyncMethod_Predict() {
      ::grpc::Service::MarkMethodAsync(0);
    }
    ~WithAsyncMethod_Predict() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Predict(::grpc::ServerContext* context, const ::google::cloud::ml::v1beta1::PredictRequest* request, ::google::api::HttpBody* response) final override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestPredict(::grpc::ServerContext* context, ::google::cloud::ml::v1beta1::PredictRequest* request, ::grpc::ServerAsyncResponseWriter< ::google::api::HttpBody>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(0, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  typedef WithAsyncMethod_Predict<Service > AsyncService;
  template <class BaseClass>
  class WithGenericMethod_Predict : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service *service) {}
   public:
    WithGenericMethod_Predict() {
      ::grpc::Service::MarkMethodGeneric(0);
    }
    ~WithGenericMethod_Predict() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Predict(::grpc::ServerContext* context, const ::google::cloud::ml::v1beta1::PredictRequest* request, ::google::api::HttpBody* response) final override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
  };
  template <class BaseClass>
  class WithStreamedUnaryMethod_Predict : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service *service) {}
   public:
    WithStreamedUnaryMethod_Predict() {
      ::grpc::Service::MarkMethodStreamed(0,
        new ::grpc::StreamedUnaryHandler< ::google::cloud::ml::v1beta1::PredictRequest, ::google::api::HttpBody>(std::bind(&WithStreamedUnaryMethod_Predict<BaseClass>::StreamedPredict, this, std::placeholders::_1, std::placeholders::_2)));
    }
    ~WithStreamedUnaryMethod_Predict() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable regular version of this method
    ::grpc::Status Predict(::grpc::ServerContext* context, const ::google::cloud::ml::v1beta1::PredictRequest* request, ::google::api::HttpBody* response) final override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    // replace default version of method with streamed unary
    virtual ::grpc::Status StreamedPredict(::grpc::ServerContext* context, ::grpc::ServerUnaryStreamer< ::google::cloud::ml::v1beta1::PredictRequest,::google::api::HttpBody>* server_unary_streamer) = 0;
  };
  typedef WithStreamedUnaryMethod_Predict<Service > StreamedUnaryService;
  typedef Service SplitStreamedService;
  typedef WithStreamedUnaryMethod_Predict<Service > StreamedService;
};

}  // namespace v1beta1
}  // namespace ml
}  // namespace cloud
}  // namespace google


#endif  // GRPC_google_2fcloud_2fml_2fv1beta1_2fprediction_5fservice_2eproto__INCLUDED
